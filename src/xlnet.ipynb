{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d817e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import XLNetTokenizer, XLNetModel, AutoTokenizer, AlbertModel, AutoModel\n",
    "\n",
    "from helper import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337043bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxLit(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Reference\n",
    "    https://machinelearningmastery.com/introduction-to-softmax-classifier-in-pytorch/\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        \n",
    "        y_hat = self(x)\n",
    "\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr = 0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "        return optimizer\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y = torch.argmax(y, dim=1)\n",
    "        y_hat = torch.argmax(self(x), dim=1)\n",
    "        accuracy = torch.sum(y == y_hat).item() / (len(y) * 1.0)\n",
    "        self.log('test_acc', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14dbd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    \"The data for multi-class classification\"\n",
    "    def __init__(self, tokenizer, cls_model, df, load_batch_size):                \n",
    "        dim_0 = df['text'].shape[0]\n",
    "        \n",
    "        docs = df['text'].tolist()\n",
    "        inputs = tokenizer(docs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "        cls_arr = []\n",
    "        for i, (x, y) in zip(tqdm(range(math.ceil(len(df) / load_batch_size))), self._get_x_y_from_df_with_batch(df, load_batch_size)):\n",
    "            cls = cls_model(**{'input_ids':inputs['input_ids'][x:y],'token_type_ids':inputs['token_type_ids'][x:y],'attention_mask':inputs['attention_mask'][x:y]}).last_hidden_state[:, 0, :].detach()\n",
    "            cls_arr.append(cls)\n",
    "        self.x = torch.concat(cls_arr)\n",
    "        \n",
    "        matrix = np.zeros((dim_0,2))\n",
    "        for i, y in enumerate(df['label'].tolist()):\n",
    "            matrix[i][y] = 1\n",
    "        self.y = torch.from_numpy(matrix)\n",
    "        self.len = dim_0\n",
    " \n",
    "    def _get_x_y_from_df_with_batch(self, df, step_size):\n",
    "        l = list(range(0, len(df), step_size))\n",
    "        for ind, _ in enumerate(l):\n",
    "            if l[ind] + step_size >= len(df):\n",
    "                yield (l[ind], len(df))\n",
    "            else:    \n",
    "                yield (l[ind], l[ind + 1])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"accessing one element in the dataset by index\"\n",
    "        return self.x[idx], self.y[idx] \n",
    " \n",
    "    def __len__(self):\n",
    "        \"size of the entire dataset\"\n",
    "        return self.len\n",
    "    \n",
    "# THIS IS LEGACY\n",
    "class DataLit(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size = 4):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def setup(self, stage):\n",
    "        df = load_dataset('../dataset/training.json', test=True)\n",
    "        dataset = Data(df[:100], 30)  # 10 > 30 > 40 yes # 4 is the best\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = utils.data.random_split(dataset,(0.8, 0.1, 0.1))\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset = self.train_dataset, batch_size = self.batch_size, shuffle=True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(dataset = self.val_dataset, batch_size = self.batch_size, shuffle=True)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(dataset = self.test_dataset, batch_size = self.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "282c3140",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dataloaders(df, tokenizer, cls_model, batch_size):\n",
    "    dataset = Data(tokenizer, cls_model, df[:100], 30)  # 10 > 30 > 40 yes # 4 is the best\n",
    "    train_dataset, val_dataset, test_dataset = utils.data.random_split(dataset,(0.8, 0.1, 0.1))\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle=True)\n",
    "    return {'train': train_dataloader, 'val': val_dataloader, 'test': test_dataloader}\n",
    "\n",
    "# THIS IS LEGACY\n",
    "# dataloader = DataLit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c847293c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertModel: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.LayerNorm.bias', 'predictions.decoder.bias', 'predictions.bias', 'predictions.decoder.weight', 'predictions.dense.weight']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:51<00:00, 12.90s/it]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "df = load_dataset('../dataset/training.json', test=True)\n",
    "\n",
    "# # XLNet: https://huggingface.co/docs/transformers/model_doc/xlnet # size = 768\n",
    "# # Might be able to use XLNetTokenizerFast\n",
    "# xlnet_tokenizer = AutoTokenizer.from_pretrained('xlnet-base-cased') # XLNetTokenizer\n",
    "# xlnet_cls_model = AutoModel.from_pretrained('xlnet-base-cased') # XLNetModel\n",
    "# xlnet_dataloaders = get_dataloaders(df, xlnet_tokenizer, xlnet_cls_model, BATCH_SIZE)\n",
    "\n",
    "# ALBERT: https://huggingface.co/docs/transformers/model_doc/albert # size = 768\n",
    "albert_tokenizer = AutoTokenizer.from_pretrained(\"albert-base-v2\")\n",
    "albert_cls_model = AutoModel.from_pretrained(\"albert-base-v2\") # AlbertModel\n",
    "albert_dataloaders = get_dataloaders(df, albert_tokenizer, albert_cls_model, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c69a32c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | linear    | Linear           | 1.5 K \n",
      "1 | softmax   | Softmax          | 0     \n",
      "2 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "1.5 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 K     Total params\n",
      "0.006     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duke/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:490: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/duke/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/duke/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/duke/anaconda3/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:281: PossibleUserWarning: The number of training batches (20) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91d01d7f476425cbb701adbdeb62034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "xlnet_model = SoftMaxLit(768, 2)\n",
    "trainer = pl.Trainer(max_epochs=5)\n",
    "\n",
    "# THIS IS LEGACY\n",
    "# trainer.fit(model, dataloader)\n",
    "\n",
    "trainer.fit(model=xlnet_model, train_dataloaders=albert_dataloaders['train'], val_dataloaders=albert_dataloaders['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e555376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/duke/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:490: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home/duke/anaconda3/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ba5b5952ce4075b1961a31aa698de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.699999988079071     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.699999988079071    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.699999988079071}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(xlnet_model, dataloaders=albert_dataloaders['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "511e6a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concat([ALBERT_Data, XLNet_Data, ...])\n",
    "\n",
    "# Ensemble model using concatenation of embedding outputs\n",
    "SVM(x: [\n",
    "    0: concat([ALBERT_embedding, XLNet_embedding, ...]),\n",
    "    1: ...\n",
    "    2: ...\n",
    "    n: ...\n",
    "], y: [0, 1, 1, 0, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2f986f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble - stacking:\n",
    "LogisticReggresion([{'x': [xlnet_y_hat, albert_y_hat, roberta_y_hat, svm_y_hat, ..., detecllm_y_hat], 'y': 1}])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
